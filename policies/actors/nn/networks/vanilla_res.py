from dataclasses import dataclass
from typing import Tuple, Optional, List, Union
import numpy as np

import torch
import torch.nn as nn

from data.synthia import Frame
from devices.light_curtain import LCReturn
from policies.actors import Pi
from policies.actors.actor import Actor
from policies.actors.nn.features.residual import NNResidualFeatures, NNResidualFeaturizer, CurtainInfo
import utils


@utils.register_class("network")
class VanillaRes(Actor):
    def __init__(self,
                 thetas: np.ndarray,
                 base_policy: Actor,
                 min_range: float,
                 max_range: float):
        """
        Args:
            thetas (np.ndarray, dtype=np.float32, shape=(C,)): thetas of the camera rays,
                in degrees and in increasing order in [-fov/2, fov/2]
            base_policy (Actor): a base policy that the residual policy policy uses.
            min_range (float): minimum range of the light curtain device
            max_range (float): maximum range of the light curtain device
        """

        self.network = CNN()
        if torch.cuda.is_available():
            self.network = self.network.cuda()

        self.base_policy = base_policy

        self.featurizer = NNResidualFeaturizer(thetas)

        self.history: List[VanillaRes.PrevOCA] = []

    @dataclass
    class PrevOCA:
        o: LCReturn  # observation at time t-1
        c: CurtainInfo  # curtain info from observations time t-1
        a: np.ndarray  # action of base policy at time t-1 using o

    @property
    def feat_dim(self) -> int:
        return self.featurizer.feat_dim

    def init_action(self,
                    init_envelope: np.ndarray) -> Tuple[np.ndarray, Optional[np.ndarray], dict]:
        """
        Args:
            init_envelope (np.ndarray, dtype=np.float32, shape=(C,)): the initial envelope provided by the environment.

        Returns:
            action (np.ndarray, dtype=np.float32, shape=(C,)): action, in terms of ranges for each camera ray.
            logp_a (Optional[np.ndarray]): log-probability of the sampled actions. None if sampling is deterministic.
            info (dict): auxiliary info generated by policy, such as a vector representation of the observation while
                         generating the action.
        """
        act, logp_a, info = self.base_policy.init_action(init_envelope)  # use base policy
        return act, None, {}

    @torch.no_grad()
    def step(self,
             obs: LCReturn) -> Tuple[np.ndarray, Optional[np.ndarray], bool, dict]:
        """
        Args:
            obs (LCReturn): observations viz return from the front light curtain.

        Returns:
            act (np.ndarray, dtype=np.float32, shape=(C,)): sampled actions -- these are absolute, not residual.
            logp_a (Optional[np.ndarray]): log-probability of the sampled actions. None if sampling is deterministic.
            control (bool): whether this policy had control of taking the action at this timestep.
                            - this will be used to determine whether to evaluate the policy at this frame.
                            - another eg. is that these are the timesteps when nn-based policies will run the network.
            info (dict): auxiliary info generated by policy, such as a vector representation of the observation while
                         generating the action.
        """
        self.network.eval()

        # convert obs to cinfo and get base policy action
        # all these quantities are for current timestep
        cinfo = self.featurizer.obs_to_curtain_info(obs)
        base_act, logp_a, _, _ = self.base_policy.step(obs)
        oca = VanillaRes.PrevOCA(obs, cinfo, base_act)

        # first timestep, base action is the final action.
        if len(self.history) == 0:
            act = base_act
            self.history.append(oca)
            return act, None, False, {}

        cpinfo_prev = self.history[0].c
        feat = self.featurizer.cinfos2feats(cpinfo_prev, cinfo, base_act)
        self.history[0] = oca  # update history

        # action from network
        pi: Pi = self.forward(feat)  # (td.distribution, batch_shape=(1,), event_shape=(C,)
        act, _ = pi.sample()  # both (1, C)
        act: np.ndarray = act.squeeze(0).cpu().numpy()  # (C,)

        logp_a  = None
        control = True
        info    = dict(
            features=feat.to_numpy().squeeze(0),  # (F,)
            base_action=base_act,  # (C,)
            pi = pi  # batch_shape=(1, C) event_shape=()
        )
        return act, logp_a, control, info

    def forward(self,
                feats: Union[torch.Tensor, NNResidualFeatures]) -> Pi:
        """
        Args:
            feats (torch.Tensor, dtype=float32, shape=(B, F)): features.

        Returns:
             pi (Pi, batch_shape=(B, C), event_shape=()): action distribution.
        """
        if type(feats) is not NNResidualFeatures:
            feats = feats.cpu().numpy()
            feats: NNResidualFeatures = NNResidualFeatures.from_numpy(feats)  # NNResidualFeatures

        pi: Pi = self.network(feats)  # batch_shape=(B, C) event_shape=()
        return pi

    def evaluate_actions(self,
                         feat: np.ndarray,
                         act : np.ndarray) -> torch.Tensor:
        """
        Args:
            feat (np.ndarray, dtype=np.float32, shape=(B, F)): batch of features.
            act (np.ndarray, dtype=np.float32, shape=(B, C)): batch of actions.

        Returns:
            logp_a (torch.Tensor): log probability of taking actions "act" by the actor under "obs", as a torch tensor.
        """
        feat = NNResidualFeatures.from_numpy(feat)  # batch_size = (B,)
        pi = self.forward(feat)  # (td.Distribution, batch_shape=(B,) event_shape=(C,))
        act = torch.from_numpy(act)  # (B, C)
        if torch.cuda.is_available():
            act = act.cuda()
        logp_a = pi.log_prob(act)  # (B,)
        return logp_a

    def reset(self):
        super().reset()
        self.history.clear()

########################################################################################################################
# region Convolutional Neural Networks
########################################################################################################################


class CNN(nn.Module):
    """Performs 1D convolutions"""
    def __init__(self):
        super().__init__()

        self.conv1 = nn.Conv1d(22, 11, kernel_size=5, padding=2)
        self.bn1   = nn.BatchNorm1d(11)
        self.conv2 = nn.Conv1d(11,  5, kernel_size=5, padding=2)
        self.bn2   = nn.BatchNorm1d(5)

        self.mu    = nn.Conv1d(5,   1, kernel_size=5, padding=2)
        self.sigma = nn.Conv1d(5,   1, kernel_size=5, padding=2)

        num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"{self.__class__.__name__}: has {num_params} parameters")

    def forward(self,
                feat: NNResidualFeatures) -> Pi:
        """
        Args:
            feat (NNResidualFeatures): features with 2D tensors, where the first dimension is the batch

        Returns:
            pi (Pi, batch_shape=(B, C), event_shape=()): action distribution
        """
        B = feat.c1.i.shape[0]  # batch size

        def pad_zeros(x: np.ndarray,
                      left: bool):
            """
            Args:
                x (np.ndarray, dtype=np.float32, shape=(B, 2): 2D vector to pad zeros to
                left (bool): whether to pad zeros to the left or not (right).
            """
            assert x.ndim == 2 and x.shape[0] == B
            if left:
                return np.hstack([np.zeros([B, 1]).astype(x.dtype), x])
            else:
                return np.hstack([x, np.zeros([B, 1]).astype(x.dtype)])

        # horizontal edges
        eh1 = feat.c1[:, 1:] - feat.c1[:, :-1]
        eh2 = feat.c2[:, 1:] - feat.c2[:, :-1]
        # vertical edges
        ev  = feat.c2 - feat.c1

        x = np.stack([
                      # intensities
                      feat.c1.i,
                      feat.c2.i,
                      # horizontal edges 1
                      pad_zeros(eh1.x, left=True ),
                      pad_zeros(eh1.x, left=False),
                      pad_zeros(eh1.z, left=True ),
                      pad_zeros(eh1.z, left=False),
                      pad_zeros(eh1.r, left=True ),
                      pad_zeros(eh1.r, left=False),
                      pad_zeros(eh1.t, left=True ),
                      pad_zeros(eh1.t, left=False),
                      # horizontal edges 2
                      pad_zeros(eh2.x, left=True),
                      pad_zeros(eh2.x, left=False),
                      pad_zeros(eh2.z, left=True),
                      pad_zeros(eh2.z, left=False),
                      pad_zeros(eh2.r, left=True),
                      pad_zeros(eh2.r, left=False),
                      pad_zeros(eh2.t, left=True),
                      pad_zeros(eh2.t, left=False),
                      # vertical edges
                      ev.x,
                      ev.z,
                      ev.r,
                      # residual actions of base policy w.r.t curtain 2
                      feat.rb - feat.c2.r
                      ], axis=1)  # (B, 22, C)

        x = torch.from_numpy(x)  # (B, 22, C)
        if torch.cuda.is_available():
            x = x.cuda()

        x = torch.relu(self.bn1(self.conv1(x)))  # (B, 11, C)
        x = torch.relu(self.bn2(self.conv2(x)))  # (B,  5, C)

        # mu: absolute
        mu = self.mu(x).squeeze(1)   # (B, C)

        rb = torch.from_numpy(feat.rb)
        if torch.cuda.is_available():
            rb = rb.cuda()

        mu = mu + rb  # (B, C)

        # sigma = 1e-5 + torch.relu(self.sigma(x)).squeeze(1)  # (B, C)
        sigma = 0.

        mu = mu.unsqueeze(-1)  # (B, C, 1)
        pi = Pi(actions=mu)
        return pi

# endregion
########################################################################################################################
