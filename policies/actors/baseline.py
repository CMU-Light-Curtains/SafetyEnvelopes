from typing import Tuple, Optional
import numpy as np
import torch

from setcpp import enforce_smoothness
from policies.actors import Pi
from policies.actors.actor import Actor

from devices.light_curtain import LCReturn


class BaselineActor(Actor):
    def __init__(self,
                 expansion: float,
                 recession: float,
                 smoothness: float,
                 f_hit_intensity_thresh: int):
        """
        Args:
            expansion (float): distance (in meters) the forecasting curtain expands by (when no hit is observed)
            recession (float): distance (in meters) the forecasting curtain receeds by (when hit is observed)
            smoothness (float): smoothing interval (in meters) the curtain is smoothed by
            f_hit_intensity_thresh (int): intensity threshold. A hit is considered to be observed iff the observed
                                          intensity is above this threshold.
        """
        # parameters
        self._expansion              = expansion  # expansion for front curtain
        self._recession              = recession  # recession for front curtain
        self._smoothness             = smoothness
        self._f_hit_intensity_thresh = f_hit_intensity_thresh  # intensity threshold for forecasting curtain

    def init_action(self,
                    init_envelope: np.ndarray) -> Tuple[np.ndarray, Optional[np.ndarray], dict]:
        """
        Simulates convergence of this policy in a frozen frame where the front curtain converges to a smoothed out
        version of the initial envelope.

        Args:
            init_envelope (np.ndarray, dtype=float32, shape=(C,)): the initial envelope provided by the environment.

        Returns:
            action (np.ndarray, dtype=float32, shape=(C,)): action, in terms of ranges for each camera ray.
            logp_a (Optional[np.ndarray]): log-probability of the sampled actions. None if sampling is deterministic.
            info (dict): auxiliary info generated by policy, such as a vector representation of the observation while
                         generating the action.
        """
        ranges = init_envelope.copy()  # (C,)

        # enforce smoothness
        ranges = np.array(enforce_smoothness(ranges, self._smoothness), dtype=np.float32)  # (C,)

        return ranges, None, {}

    def step(self,
             obs: LCReturn) -> Tuple[np.ndarray, Optional[np.ndarray], bool, dict]:
        """
        Args:
            obs (LCReturn): observations viz return from the front light curtain.

        Returns:
            act (np.ndarray, dtype=float32, shape=(C,)): sampled actions.
            logp_a (Optional[np.ndarray]): log-probability of the sampled actions. None if sampling is deterministic.
            control (bool): whether this policy had control of taking the action at this timestep.
                            - this will be used to determine whether to evaluate the policy at this frame.
                            - another eg. is that these are the timesteps when nn-based policies will run the network.
            info (dict): auxiliary info generated by policy, such as a vector representation of the observation while
                         generating the action.
        """
        f_return = obs

        # Get hits on front curtain
        f_curtain = f_return.lc_ranges.copy()  # (C,)
        f_hits = f_return.bev_hits(ithresh=self._f_hit_intensity_thresh)  # (C,)

        ################################################################################################################
        # Expand+Recede+Smooth frontier
        ################################################################################################################

        # Expansion
        f_curtain[~f_hits] += self._expansion

        # Recession
        f_curtain[ f_hits] -= self._recession

        # Enforce smoothness
        f_curtain = np.array(enforce_smoothness(f_curtain, self._smoothness), dtype=np.float32)  # (C,)

        pi = Pi(actions=torch.from_numpy(f_curtain[None, :, None]))  # batch_shape=(1, C) event_shape=()
        info = dict(pi=pi)

        action, logp_a, control = f_curtain, None, True  # logp_a is None since this policy is deterministic
        return action, logp_a, control, info

    def forward(self,
                obs: torch.Tensor) -> torch.distributions.Distribution:
        """
        Args:
            obs (torch.Tensor): observation in torch tensors.

        Returns:
            pi (torch.distributions.Distribution): predicted action distribution of the policy.
        """
        raise NotImplementedError

    def evaluate_actions(self,
                         obs: torch.Tensor,
                         act: torch.Tensor) -> torch.Tensor:
        """
        Args:
            obs (torch.Tensor): observation in torch tensors.
            act (torch.Tensor): actions in torch tensors.

        Returns:
            logp_a (torch.Tensor): log probability of taking actions "act" by the actor under "obs", as a torch tensor.
        """
        raise NotImplementedError

    def reset(self):
        pass
