from abc import ABC, abstractmethod
import numpy as np
import torch
from typing import Optional, Tuple

from data.synthia import Frame
from devices.light_curtain import LCReturn
from policies.actors import Pi


class Actor(ABC):
    @abstractmethod
    def init_action(self,
                    state: Frame) -> Tuple[np.ndarray, Optional[np.ndarray], dict]:
        """
        Args:
            state (Frame): the state of the world represented by a Frame from the gt_state_device.

        Returns:
            action (np.ndarray, dtype=float32, shape=(C,)): action, in terms of ranges for each camera ray.
            logp_a (Optional[np.ndarray]): log-probability of the sampled actions. None if sampling is deterministic.
            info (dict): auxiliary info generated by policy, such as a vector representation of the observation while
                         generating the action.
        """
        raise NotImplementedError

    @abstractmethod
    def step(self,
             obs: LCReturn) -> Tuple[np.ndarray, Optional[np.ndarray], bool, dict]:
        """
        Args:
            obs (LCReturn): observations viz return from the front light curtain.
        
        Returns:
            act (np.ndarray, dtype=float32, shape=(C,)): sampled actions.
            logp_a (Optional[np.ndarray]): log-probability of the sampled actions. None if sampling is deterministic.
            control (bool): whether this policy had control of taking the action at this timestep.
                            - this will be used to determine whether to evaluate the policy at this frame.
                            - another eg. is that these are the timesteps when nn-based policies will run the network.
            info (dict): auxiliary info generated by policy, such as a vector representation of the observation while
                         generating the action.
        """
        raise NotImplementedError

    def forward(self,
                feats: torch.Tensor) -> Pi:
        """
        Args:
            feats (torch.Tensor, dtype=float32, shape=(B, *F)): features of dimensions *F.
        
        Returns:
            pi (Pi, device=cuda, batch_shape=(B, C), event_shape=()): action distribution.
        """
        raise NotImplementedError

    def reset(self):
        """
        Generally, actors can maintain a history of observations from the environment, and actions can be a function of
        the entire history. Reset is used to clear any such histories.
        """
        pass
